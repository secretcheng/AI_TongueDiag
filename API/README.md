# ğŸš€ Tongue Multi-Modal Large Model API Service

> ğŸŒŸ Welcome to the Multi-Modal Large Model API Service! This guide will help you get started with our powerful API for integrating Tongue-Multi-Modal LLM. Dive in to explore how to set up your Python environment, install necessary packages, and use the API with example code.

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![AI-Powered](https://img.shields.io/badge/AI-Powered-red.svg)]()

</div>

## ğŸ“š Overview

This module is designed for developers to quickly deploy multi-modal large language models locally and leverage API services for intelligent tongue image analysis.

---

## ğŸ“¦ Installation

### 1. Source Code Installation (Using Conda)

Due to variations in GPU configurations, this guide provides specific installation commands instead of a `requirements.txt` file.

#### 1.1 Python Environment

```bash
# Create a new conda environment
conda create -n aitongue python=3.10 -y
conda activate aitongue

# Install PyTorch according to your machine's CUDA version
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --default-timeout=100

# Install LLaMA Factory
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics,deepspeed]"

# Install qwen-vl-utils
pip install qwen-vl-utils[decord]
```

#### 1.2 Model Checkpoint Download
This model checkpoint: https://huggingface.co/secretcheng/qwen2vl_tongue_test is recommended, it is fine-tuned on Private Tongue image data.
Also you can use the original Qwen2-VL model: https://huggingface.co/Qwen/Qwen-VL-7B-Instruct. However, the format may not be consistent with the output, and the output content is slightly different.

#### 1.3 API Service Setup
> **Use LLaMA Factory**  

Configure the `qwen2-vl-inference.yaml` file. A template is provided below:

```yaml
model_name_or_path: xxx  # Provide the path to your qwen2-vl model
template: qwen2_vl
infer_backend: huggingface  

temperature: 0.01
top_p: 0.1
top_k: 20
max_new_tokens: 8192
cutoff_len: 12800
```

Start the API service with the following command:

```bash
API_PORT=8000 llamafactory-cli api qwen2-vl-inference.yaml
```

> **Use FastAPI+transformers**

```bash
python main.py
```

> **Use vllm**  

`qwen2-vl-inference.yaml` has infer_backend argument, change it to vllm  
or use this command to start the API service:
```bash
python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-VL-7B-Instruct --model xxx --gpu_memory_utilization 0.90 --max_model_len 12800 --host localhost --port 8000
```


### 2. Docker Installation

Docker installation is coming soon. Stay tuned!

---

## ğŸš€ Usage
Each framework starts with a slightly different API call, as described in `test_api.ipynb`. 
The following examples use the **LLaMA Factory** framework as an example.
### 1. Using cURL

```bash
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d "{
    "model": "xxx",
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "æè¿°è¿™ä¸ªèˆŒè±¡å›¾ç‰‡"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "xxx.jpg"
            }
          }
        ]
      }
    ],
  }"

```

### 2. Using Python(Recommended)
we also provide an example of how to use the API with Python. See details in `test_api.ipynb`
```python
import os
from openai import OpenAI

instruction_prompt = """
è¯·æ ¹æ®ç»™å®šçš„èˆŒè±¡å›¾ç‰‡åŠå…¶æ ‡ç­¾æ•°æ®ï¼Œç”Ÿæˆä¸€æ®µæè¿°èˆŒè±¡ç‰¹å¾çš„æ–‡å­—ã€‚æè¿°åº”åŒ…å«ä»¥ä¸‹å¿…å¡«é¡¹ï¼š
- èˆŒè´¨é¢œè‰²
- èˆŒä½“å¤§å°ï¼ˆèƒ–ã€èƒ–ç˜¦é€‚ä¸­ã€ç˜¦ï¼‰
- èˆŒé¢æ¹¿æ¶¦æˆ–å¹²ç‡¥ (æ¹¿æ¶¦ã€åæ¶¦ã€æ¶¦ç‡¥é€‚ä¸­ã€åç‡¥ã€å¹²ç‡¥)
- èˆŒè¾¹æœ‰æ— é½¿ç—• ï¼ˆæœ‰é½¿ç—•ã€æ— é½¿ç—•ï¼‰
- èˆŒé¢æœ‰æ— è£‚çº¹ã€æœ‰æ— ç‚¹åˆº ï¼ˆæœ‰è£‚çº¹ã€æ— è£‚çº¹ã€æœ‰ç‚¹åˆºã€æ— ç‚¹åˆºï¼Œä¸¤ç§ç—‡çŠ¶éƒ½éœ€è¦æè¿°ï¼‰
- èˆŒä½“è€å«© ï¼ˆå«©ã€åå«©ã€è€å«©é€‚ä¸­ã€åè€ã€è€ï¼‰
- èˆŒè‹”è¦†ç›–èŒƒå›´ ï¼ˆå…¨è‹”ã€èˆŒæ ¹éƒ¨ã€èˆŒä¸­éƒ¨ã€èˆŒå°–éƒ¨ã€èˆŒå·¦éƒ¨ã€èˆŒå³éƒ¨ã€æ— åè‹”ï¼‰
- èˆŒè‹”è¡¨ç°å½¢å¼

èˆŒè‹”çš„è¡¨ç°å½¢å¼å¿…é¡»æŒ‰ç…§ä»¥ä¸‹è§„åˆ™æè¿°ï¼š
- èˆŒè‹”é¢œè‰² + åšè–„ + è…»è…ï¼ˆå¦‚ï¼šè‹”é»„åšè…»ï¼‰

"""

client = OpenAI(
        api_key="{}".format(os.environ.get("API_KEY", "0")),
        base_url="http://localhost:{}/v1".format(os.environ.get("API_PORT", 8000)),
    )
messages = []
messages.append(
    {
        "role": "system",
        "content": instruction_prompt,
    }
)
messages.append(
    {
        "role": "user",
        "content": [
            {"type": "text", "text": instruction_prompt},
            {"type": "text", "text": "æè¿°è¿™ä¸ªèˆŒè±¡å›¾ç‰‡"},
            {
                "type": "image_url",
                "image_url": {"url": "/home/sc/LLM_Learning/AI_TongueDiag/data/images/0003-1.jpg"},
            },
        ],
    }
)
result = client.chat.completions.create(messages=messages, model="Qwen2-VL-7B-Instruct")
messages.append(result.choices[0].message)
print(result.choices[0].message.content)
```
è¾“å‡ºæ ·ä¾‹å¦‚ä¸‹:
```markdown
æ ¹æ®æ‚¨è¾“å…¥çš„å›¾ç‰‡ï¼Œæ¨¡å‹åˆ†æçš„èˆŒè±¡ç»“æœæ˜¯ï¼š
  -èˆŒè´¨é¢œè‰²ï¼šæ·¡çº¢ï¼›
  -èˆŒä½“å¤§å°ï¼šèƒ–ç˜¦é€‚ä¸­ï¼›
  -èˆŒé¢æ¹¿æ¶¦ç¨‹åº¦ï¼šæ¶¦ç‡¥é€‚ä¸­ï¼›
  -èˆŒè¾¹ï¼šæ— é½¿ç—•ï¼›
  -èˆŒé¢ï¼šæ— è£‚çº¹ã€æ— ç‚¹åˆºï¼›
  -èˆŒè´¨è€å«©ç¨‹åº¦ï¼šè€å«©é€‚ä¸­ï¼›
  -èˆŒè‹”ä¸»è¦è¦†ç›–æƒ…å†µï¼šæ— åè‹”ï¼›
  -èˆŒè‹”è¡¨ç°å½¢å¼ï¼šè‹”åç™½åšè–„é€‚ä¸­æ— è…»è…
```

---

## ğŸ› ï¸ Contributing

We welcome contributions to enhance the API service! Feel free to submit issues or pull requests on [GitHub](https://github.com/your-repo-link).

## ğŸ“œ License

This project is licensed under the MIT License. 

---

Enjoy exploring the capabilities of the Tongue Multi-Modal Large Model API Service! ğŸŒŸ





<div align="center">
Built with â¤ï¸ for advancing Traditional Chinese Medicine

```
     _    ___   ___              _____  _____ __  __ 
    / \  |_ _| |  ___|__  _ __ |_   _|/ ____|  \/  |
   / _ \  | |  | |_ / _ \| '__|  | | | |    | \  / |
  / ___ \ | |  |  _| (_) | |     | | | |    | |\/| |
 /_/   \_\___| |_|  \___/|_|     |_|  \____|_|  |_|
```
</div>

